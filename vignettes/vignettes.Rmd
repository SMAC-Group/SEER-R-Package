---
title: "swag vignette"
author: "Gaetan Bakalli, Samuel Orso and Cesare Miglioli"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to swag}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction 

**swag** is a package that train a meta-learning procedure that combines screening and wrapper methods to find a set of extremely low-dimensional attribute combinations. **swag** works on top of the **caret** package and proceeds in a forward-step manner. More specifically, it builds and tests learners starting from very few attributes until it includes a maximal number of attributes by increasing the number of attributes at each step. Hence, for each fixed number of attributes, the algorithm tests various (randomly selected) learners and picks those with the best performance in terms of training error. Throughout, the algorithm uses the information coming from the best learners at the previous step to build and test learners in the following step. In the end, it outputs a set of strong low-dimensional learners.

## Installation

First you need to install the *devtools* package. Now you are able to get the **swag** directly from Github with the following code:

```{r, eval=FALSE}

devtools::install_github("SMAC-Group/SWAG-R-Package")

library(swag) #load the new package

```

## Quick Start

The purpose of this section is to give a general sense of the package, including the components, what they do and some basic usage. We will briefly go over the main functions, see the basic operations and have a look at the outputs. You may have a better idea after this section regarding what functions are available, which one to choose, or at least where to seek help. More details are given in later sections.

We propose to use a dataset readily available on the package mlbench. The dataset consists of a sample of $699$ patients and the objective is to predict whether a new patient has a malignant tumour given a collection of $9$ features. We can start by splitting the data in training and test set. Alternatively you can either load directly your own data or use those saved in the workspace following exactly the same steps outlined in the next paragraphs. 

```{r, eval=FALSE}

# After having installed the mlbench package

data(BreastCancer, package = "mlbench")

# Pre-processing of the data

y <- BreastCancer$Class # response variable

x <- as.matrix(BreastCancer[setdiff(names(BreastCancer),c("Id","Class"))]) # features

# remove missing values

id <- which(apply(X,1,function(x) sum(is.na(x)))>0)
y <- y[-id]
x <- x[-id,]
x <- apply(x,2,as.numeric)

# Training and test set

set.seed(180) # for replication

ind <- sample(1:dim(x)[1],dim(x)[1]*0.2)  

y_test <- y[ind]

y_train <- y[-ind]

x_test <- x[ind]

x_train <-x[-ind]


```


Now we are ready to train the **swag** on the breast cancer dataset. As explained before, we build on the framework of the package **caret** thus expert users of this package will find the whole implementation easier. In any case, we will explain all the important steps needed for **swag** and we suggest to the interested reader the following detailed e-book: [caret](http://topepo.github.io/caret/index.html). The first step is to fix the meta-parameters of the procedures: $p_{max}$, $\alpha$ and $m$. As the name suggests, $p_{max}$ is the maximum dimension of attributes that the user wants to be input in a learner, $\alpha$ is a performance quantile which represents the percentage of learners which are selected at each dimension $ 1 \leq \hat{p} \leq p_{max}$ and finally $m$ which represent the maximum numbers of learners which will be trained at each dimension $\hat{p} > 1$ (this because we train all $p$ learners of dimension $1$). The *swagcontrol()* function 

```{r, eval=FALSE}

library(caret)

trial <- swagControl(pmax = 3,alpha = 0.3,m = 100,seed = 163L, verbose = T)

# Test with glmnet
test_swag_glmnet <- swag(
  # arguments for swag
  x = x, y = y, control = swagControl(alpha=.5,verbose=TRUE),
  # arguments for caret
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 1, allowParallel = F),
  metric = "Accuracy",
  method = "glmnet",
  tuneGrid=expand.grid(alpha = 1, lambda = seq(0,.35,length.out=10)),
  family="binomial",
  # dynamically modify arguments for caret
  caret_args_dyn = function(list_arg,iter){
    if(iter==1){
      list_arg$method = "glm"
      list_arg$tuneGrid = NULL
    }
    list_arg
  }
)

```



