<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Gaetan Bakalli, Samuel Orso, Cesare Miglioli, Roberto Molinari and Stephane Guerrier" />

<meta name="date" content="2020-06-11" />

<title>swag vignette</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">swag vignette</h1>
<h4 class="author">Gaetan Bakalli, Samuel Orso, Cesare Miglioli, Roberto Molinari and Stephane Guerrier</h4>
<h4 class="date">2020-06-11</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><strong>swag</strong> is a package that trains a meta-learning procedure that combines screening and wrapper methods to find a set of extremely low-dimensional attribute combinations. <strong>swag</strong> works on top of the <strong>caret</strong> package and proceeds in a forward-step manner. More specifically, it builds and tests learners starting from very few attributes until it includes a maximal number of attributes by increasing the number of attributes at each step. Hence, for each fixed number of attributes, the algorithm tests various (randomly selected) learners and picks those with the best performance in terms of training error. Throughout, the algorithm uses the information coming from the best learners at the previous step to build and test learners in the following step. In the end, it outputs a set of strong low-dimensional learners.</p>
<p>Given the above intuitive description, we now provide a more formal introduction and, for this reason, we define some basic notation. Let <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> denote the response and <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> denote an attribute matrix with <span class="math inline">\(n\)</span> instances and <span class="math inline">\(p\)</span> attributes, the latter being indexed by a set <span class="math inline">\(\mathcal{S} := \{1, ... , p\}\)</span>. In addition, a generic learning mechanism is denoted as <span class="math inline">\(\mathcal{L}:= \mathcal{L}(\mathbf{y}, \mathbf{X})\)</span> with <span class="math inline">\(l\)</span> denoting a general learner which is built by using (i) the learning mechanism <span class="math inline">\(\mathcal{L}\)</span> and (ii) a subset of attributes in <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>Before getting started, install the <strong>devtools</strong> package. Then, <strong>swag</strong> can be directly obtained from Github with the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;SMAC-Group/SWAG-R-Package&quot;</span>)

<span class="kw">library</span>(swag) <span class="co">#load the new package</span></code></pre></div>
</div>
<div id="start-with-swag" class="section level2">
<h2>Start with swag!</h2>
<p>The purpose of this section is to give a general sense of the package, including the components, what they do and some basic usage. We will briefly go over the main functions, see the basic operations and have a look at the outputs. You may have a better idea after this section regarding what functions are available, which one to choose, or at least where to seek for more detailed informations.</p>
<p>We propose to use a dataset readily available from the package <strong>mlbench</strong>. The dataset consists of a cohort of <span class="math inline">\(n = 699\)</span> patients and the objective is to predict whether a new patient has a malignant tumour given a collection of <span class="math inline">\(p = 9\)</span> attributes (tape <code>?mlbench::BreastCancer</code> for more details). We can start by splitting the data into training and test set. Alternatively you can either load directly your own data or use those saved in the workspace following exactly the same steps outlined in the next paragraphs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># After having installed the mlbench package</span>

<span class="kw">data</span>(BreastCancer, <span class="dt">package =</span> <span class="st">&quot;mlbench&quot;</span>)

<span class="co"># Pre-processing of the data</span>
y &lt;-<span class="st"> </span>BreastCancer<span class="op">$</span>Class <span class="co"># response variable</span>
x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(BreastCancer[<span class="kw">setdiff</span>(<span class="kw">names</span>(BreastCancer),<span class="kw">c</span>(<span class="st">&quot;Id&quot;</span>,<span class="st">&quot;Class&quot;</span>))]) <span class="co"># features</span>

<span class="co"># remove missing values and change to 'numeric'</span>
id &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">apply</span>(x,<span class="dv">1</span>,<span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))<span class="op">&gt;</span><span class="dv">0</span>)
y &lt;-<span class="st"> </span>y[<span class="op">-</span>id]
x &lt;-<span class="st"> </span>x[<span class="op">-</span>id,]
x &lt;-<span class="st"> </span><span class="kw">apply</span>(x,<span class="dv">2</span>,as.numeric)

<span class="co"># Training and test set</span>
<span class="kw">set.seed</span>(<span class="dv">180</span>) <span class="co"># for replication</span>
ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">dim</span>(x)[<span class="dv">1</span>],<span class="kw">dim</span>(x)[<span class="dv">1</span>]<span class="op">*</span><span class="fl">0.2</span>)  
y_test &lt;-<span class="st"> </span>y[ind]
y_train &lt;-<span class="st"> </span>y[<span class="op">-</span>ind]
x_test &lt;-<span class="st"> </span>x[ind,]
x_train &lt;-x[<span class="op">-</span>ind,]</code></pre></div>
<p>Now we are ready to train the <strong>swag</strong> on the breast cancer dataset. As previously mentionned, we build upon the framework of the package <strong>caret</strong> thus experimented users of this package will find the whole implementation easier. In any case, we will explain in detail all the important steps needed for <strong>swag</strong> and we suggest to the interested reader the following detailed e-book: <a href="http://topepo.github.io/caret/index.html">caret</a>.</p>
<p>The first step is to fix the meta-parameters of the <strong>swag</strong> procedures: <span class="math inline">\(p_{max}\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(m\)</span>. As the name suggests, <span class="math inline">\(p_{max}\)</span> is the maximum dimension of attributes that the user wants to be input in a generic <span class="math inline">\(\mathcal{L}(\mathbf{y}, \mathbf{X})\)</span>. Based on this parameter, the <em>swag</em> aims at exploring the space of attributes in order to find sets of learners using <span class="math inline">\(\hat{p}\)</span> attributes (<span class="math inline">\(1 \leq \hat{p} \leq p_{\text{max}}\)</span>) with extremely high predictive power. To do so, the algorithm makes use of the step-wise screening procedure described briefly in the introduction. Another key element is <span class="math inline">\(\alpha\)</span>: a performance quantile which represents the percentage of learners which are selected at each dimension <span class="math inline">\(1 \leq \hat{p} \leq p_{max}\)</span>. Finally we need to choose <span class="math inline">\(m\)</span> which represent the maximum numbers of learners which will be trained at each dimension <span class="math inline">\(\hat{p} &gt; 1\)</span> (i.e. we train all <span class="math inline">\(p\)</span> learners of dimension <span class="math inline">\(1\)</span>). We can fix all these meta-parameters, together with a seed for replicability purposes and <code>verbose = TRUE</code> to get a message as each dimension is completed, thanks to the <em>swagcontrol()</em> function which behaves similarly to the <code>trControl =</code> argument of <strong>caret</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Meta-parameters chosen for the breast cancer dataset</span>
swagcon &lt;-<span class="st"> </span><span class="kw">swagControl</span>(<span class="dt">pmax =</span> 4L, 
                       <span class="dt">alpha =</span> <span class="fl">0.5</span>, 
                       <span class="dt">m =</span> 20L,
                       <span class="dt">seed =</span> 163L, <span class="co">#for replicability</span>
                       <span class="dt">verbose =</span> T <span class="co">#keeps track of completed dimensions</span>
                       )

<span class="co"># Given the low dimensional dataset, we can afford a wider search by fixing alpha = 0.5 as a smaller alpha may also stop the training procedure earlier than expected.</span></code></pre></div>
<p>If you do not specify these values, you will get the default values: <span class="math inline">\(p_{max} = 3\)</span>, <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(m = 100\)</span>. Ideally, with unlimited computing power, <span class="math inline">\(p_{max}\)</span> and <span class="math inline">\(m\)</span> should be as large as possible, i.e. <span class="math inline">\(p_{\text{max}} = p\)</span> and <span class="math inline">\(m = \binom{p}{\lceil \frac{p}{2}\rceil}\)</span>. However, this is typically unrealistic and therefore the decision of these parameters must be based mainly on interpretability/replicability requirements as well as available computing power and time constraints. Below are some rules-of-thumb for the choice of these parameters:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{p_{\text{max}}}\)</span>: Fixing the available computing power and the efficiency of the learning mechanism <span class="math inline">\(\mathcal{L}\)</span>, this parameter will depend on the total dimension of the problem <span class="math inline">\(p\)</span>. Indeed, the goal of <strong>swag</strong> is to find extremely small dimensional learners. Therefore, even with very large <span class="math inline">\(p\)</span>, one could always fix this parameter within a range of 5-20 (or smaller) for interpretability and/or replicability purposes. In addition, if an embedded method is computationally efficient to compute on the entire dataset, this parameter could be the number of selected attributes through this method (given computational constraints). Another criterion, when working with binary classification problems, is to use the <em>Event Per Variable</em> (EPV) rule. In future work, this parameter can be implicitly determined by the algorithm based on the training error quantile (or other metric) thereby defining <span class="math inline">\(p_{\text{max}}\)</span> as the attribute dimension where the training error curve stops decreasing significantly similarly to the <em>scree plot</em> in factor or principal component analysis.</p></li>
<li><p><span class="math inline">\(\boldsymbol{\alpha}\)</span>: this parameter is related to the maximum number of learners <span class="math inline">\(m\)</span>. The larger <span class="math inline">\(\alpha\)</span>, the more the attribute space is explored. Ideally, we want to choose a small <span class="math inline">\(\alpha\)</span> since we would want to select strong learners (with extremely low training error) and this is possible if <span class="math inline">\(m\)</span> is large enough. Generally good values for <span class="math inline">\(\alpha\)</span> are <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.05\)</span>, implying that (roughly) 1% or 5% of the <span class="math inline">\(m\)</span> learners are selected at each step.</p></li>
<li><p><span class="math inline">\(\mathbf{m}\)</span>: Fixing the available computing power and the efficiency of the learning mechanism <span class="math inline">\(\mathcal{L}\)</span>, this parameter will determine the proportion of attribute space that will be explored by the algorithm. We know that it depends on the size of the problem <span class="math inline">\(p\)</span> since we necessarily have <span class="math inline">\(m \geq p\)</span> for the screening step with models of unitary dimension. In addition, this parameter needs to be chosen considering the performance percentile <span class="math inline">\(\alpha\)</span>: if <span class="math inline">\(m\)</span> is small and <span class="math inline">\(\alpha\)</span> is small, then the number of strong learners being selected could be extremely low (possibly zero). In general, we would want a large <span class="math inline">\(m\)</span> (so that <span class="math inline">\(\alpha\)</span> can eventually be chosen to be very small) and, fixing <span class="math inline">\(p^\star\)</span> as the number of attributes released from the first screening, a rule-of-thumb is to set <span class="math inline">\(m = \binom{p^\star}{2}\)</span> (or close to it) in order to explore the entire (or most of the) subspace of two-dimensional learners generated by <span class="math inline">\(p^\star\)</span>.</p></li>
</ul>
<p>Having set-up the meta-parameters as explained above, we are now ready to train the <strong>swag</strong>. We start with the Support Vector Machine learner, both linear and radial, as displayed by the chunk below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### SVM Linear Learner <span class="al">###</span>
train_swag_svml &lt;-<span class="st"> </span><span class="kw">swag</span>(
  <span class="co"># arguments for swag</span>
  <span class="dt">x =</span> x_train, 
  <span class="dt">y =</span> y_train, 
  <span class="dt">control =</span> swagcon,
  <span class="dt">auto_control =</span> <span class="ot">FALSE</span>,
  <span class="co"># arguments for caret</span>
  <span class="dt">trControl =</span> caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">1</span>, <span class="dt">allowParallel =</span> F),
  <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
  <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>,  <span class="co"># Use method = &quot;svmRadial&quot; to train this specific learner</span>
  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>)
)
<span class="co">#&gt; [1] &quot;Dimension explored: 1 - CV errors at alpha: 0.115&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 2 - CV errors at alpha: 0.0549&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 3 - CV errors at alpha: 0.0403&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 4 - CV errors at alpha: 0.0394&quot;</span></code></pre></div>
<p>The only difference with respect to the classic <strong>caret</strong> train function, is the specification of the <strong>swag</strong> arguments which have been explained previously. To give an overview, in the above chunk for the <em>svmLinear</em> learner, we have chosen to fix 10-fold cross-validation repeated 1 time as our estimator of the out-of-sample accuracy that we selected as our metric to evaluate the classifier’s performance. For this specific case, we have chosen to center and rescale the data, as usually done for svms, and, the parameter that controls the margin in svms is automatically fixed at unitary value (i.e. <span class="math inline">\(c=1\)</span>). For further explanations, we redirect the interested reader to the detailed e-book: <a href="http://topepo.github.io/caret/index.html">caret</a>.</p>
<p>Let’s have a look at the typical output of a <strong>swag</strong> training object for the <em>svmLinear</em> learner:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_swag_svml<span class="op">$</span>CVs  
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt; [1] 0.14094276 0.06959836 0.07499399 0.15157407 0.10811688 0.08592593 0.11502886</span>
<span class="co">#&gt; [8] 0.12070707 0.22122896</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt;  [1] 0.05107744 0.06225950 0.03852213 0.05492304 0.06030544 0.04377104</span>
<span class="co">#&gt;  [7] 0.05108225 0.06212121 0.07485570 0.05491582</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[3]]</span>
<span class="co">#&gt; [1] 0.04010101 0.04761063 0.03848846 0.04030784 0.04575758 0.04016835 0.03841991</span>
<span class="co">#&gt; [8] 0.04387205 0.05105099</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[4]]</span>
<span class="co">#&gt; [1] 0.03464646 0.04572751 0.04030664 0.03852213</span>

<span class="co"># A list which contains the cv training errors of each learner explored in a given dimension</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_swag_svml<span class="op">$</span>VarMat 
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]</span>
<span class="co">#&gt; [1,]    1    2    3    4    5    6    7    8    9</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span>
<span class="co">#&gt; [1,]    2    2    2    2    3    3    3    5    5     6</span>
<span class="co">#&gt; [2,]    3    5    6    7    5    6    7    6    7     7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[3]]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]</span>
<span class="co">#&gt; [1,]    2    2    2    3    2    2    3    3    5</span>
<span class="co">#&gt; [2,]    3    3    6    6    3    5    5    5    6</span>
<span class="co">#&gt; [3,]    6    7    7    7    5    6    6    7    7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[4]]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    2    2    2    3</span>
<span class="co">#&gt; [2,]    3    3    5    5</span>
<span class="co">#&gt; [3,]    6    5    6    6</span>
<span class="co">#&gt; [4,]    7    6    7    7</span>

<span class="co"># A list which contrains a matrix, for each dimension, with the attributes tested at that step </span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_swag_svml<span class="op">$</span>cv_alpha 
<span class="co">#&gt; [1] 0.11502886 0.05491943 0.04030784 0.03941438</span>

<span class="co"># The cut-off cv training error, at each dimension, determined by the choice of alpha</span></code></pre></div>
<p>The other two learners that we have carefully implemented on <strong>swag</strong> are: lasso (<strong>glmnet</strong> package required) and random forest (<strong>party</strong> package required). The training phase for these learners, differs a little with respect to the svm one. We start looking at the lasso:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Lasso Learner <span class="al">###</span>
train_swag_lasso &lt;-<span class="st"> </span><span class="kw">swag</span>(
  <span class="co"># arguments for swag</span>
  <span class="dt">x =</span> x, 
  <span class="dt">y =</span> y, 
  <span class="dt">control =</span> swagcon,
  <span class="dt">auto_control =</span> <span class="ot">FALSE</span>,
  <span class="co"># arguments for caret</span>
  <span class="dt">trControl =</span> caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">1</span>, <span class="dt">allowParallel =</span> F),
  <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
  <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,
  <span class="dt">tuneGrid=</span><span class="kw">expand.grid</span>(<span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>,.<span class="dv">35</span>,<span class="dt">length.out=</span><span class="dv">10</span>)),
  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>,
  <span class="co"># dynamically modify arguments for caret</span>
  <span class="dt">caret_args_dyn =</span> <span class="cf">function</span>(list_arg,iter){
    <span class="cf">if</span>(iter<span class="op">==</span><span class="dv">1</span>){
      list_arg<span class="op">$</span>method =<span class="st"> &quot;glm&quot;</span>
      list_arg<span class="op">$</span>tuneGrid =<span class="st"> </span><span class="ot">NULL</span>
    }
    list_arg
  }
)
<span class="co">#&gt; [1] &quot;Dimension explored: 1 - CV errors at alpha: 0.1214&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 2 - CV errors at alpha: 0.0616&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 3 - CV errors at alpha: 0.0483&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 4 - CV errors at alpha: 0.041&quot;</span></code></pre></div>
<p>The newly introduced argument <code>caret_args_dyn</code> enables the user to modify the hyper-parameters related to a given learner in a dynamic way since they can change as the dimension <span class="math inline">\(\hat{p}\)</span> grows up to the desired <span class="math inline">\(p_{max}\)</span>. In the case of lasso, <code>caret_args_dyn =</code> clarifies that if we are training unitary <span class="math inline">\(\mathcal{L}(\mathbf{y}, \mathbf{x}_{i})\)</span> (i.e. learners with a unique attribute <span class="math inline">\(\mathbf{x}_{i} \; \forall \;i \in \mathcal{S}\)</span> ) then we will use a logistic regression (i.e. an un-penalized learner). This modification is in fact due to the implementation of the lasso in <strong>glmnet</strong> package as one attribute is not accepted (see this <a href="https://stackoverflow.com/questions/29231123/why-cant-pass-only-1-coulmn-to-glmnet-when-it-is-possible-in-glm-function-in-r/59414707#59414707">discussion</a>). On the other hand, for the random forest case, we would ideally want to adapt the <em>mtry</em> hyper-parameter as the dimension grows. In the example below, we fix <span class="math inline">\(mtry = \sqrt{\hat{p}}\)</span> as it is usually done in practice.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Random Forest Learner <span class="al">###</span>
test_swag_rf &lt;-<span class="st"> </span><span class="kw">swag</span>(
  <span class="co"># arguments for swag</span>
  <span class="dt">x =</span> x, 
  <span class="dt">y =</span> y, 
  <span class="dt">control =</span> swagcon,
  <span class="dt">auto_control =</span> <span class="ot">FALSE</span>,
  <span class="co"># arguments for caret</span>
  <span class="dt">trControl =</span> caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">1</span>, <span class="dt">allowParallel =</span> F),
  <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
  <span class="co"># dynamically modify arguments for caret</span>
  <span class="dt">caret_args_dyn =</span> <span class="cf">function</span>(list_arg,iter){
    list_arg<span class="op">$</span>tuneGrid =<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.mtry=</span><span class="kw">sqrt</span>(iter))
    list_arg
  }
)
<span class="co">#&gt; [1] &quot;Dimension explored: 1 - CV errors at alpha: 0.0996&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 2 - CV errors at alpha: 0.0534&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 3 - CV errors at alpha: 0.0461&quot;</span>
<span class="co">#&gt; [1] &quot;Dimension explored: 4 - CV errors at alpha: 0.0425&quot;</span></code></pre></div>
<p>Indeed a nice feature of <strong>swag</strong>, that derives from its building block <strong>caret</strong>, is that you can tailor the learning arguments of <em>swag()</em> as you like introducing for example grids for the hyper-parameters specific of a given learner or update these grids as the dimension increases. This gives to the user a wide range of possibilities and a lot of flexibility in the training phase.</p>
<p>To conclude this opening section, we present the usual <em>predict()</em> function which can be applied to a <strong>swag</strong> trained object similarly to many other packages in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># best learner predictions </span>
<span class="co"># if `newdata` is not specified, then predict gives predictions based on the training </span>
<span class="co"># sample</span>

<span class="kw">sapply</span>(<span class="kw">predict</span>(<span class="dt">object =</span> train_swag_svml), <span class="cf">function</span>(x) <span class="kw">head</span>(x))
<span class="co">#&gt; $predictions</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,]    2</span>
<span class="co">#&gt; [2,]    1</span>
<span class="co">#&gt; [3,]    2</span>
<span class="co">#&gt; [4,]    1</span>
<span class="co">#&gt; [5,]    2</span>
<span class="co">#&gt; [6,]    1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models</span>
<span class="co">#&gt; $models[[1]]</span>
<span class="co">#&gt; [1] 2 3 6 7</span>

<span class="co"># best learner predictions </span>
best_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> train_swag_svml, 
                     <span class="dt">newdata =</span> x_test)

<span class="kw">sapply</span>(best_pred, <span class="cf">function</span>(x) <span class="kw">head</span>(x))
<span class="co">#&gt; $predictions</span>
<span class="co">#&gt;      [,1]</span>
<span class="co">#&gt; [1,]    1</span>
<span class="co">#&gt; [2,]    1</span>
<span class="co">#&gt; [3,]    1</span>
<span class="co">#&gt; [4,]    2</span>
<span class="co">#&gt; [5,]    1</span>
<span class="co">#&gt; [6,]    1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models</span>
<span class="co">#&gt; $models[[1]]</span>
<span class="co">#&gt; [1] 2 3 6 7</span>

<span class="co"># predictions for a given dimension </span>

dim_pred &lt;-<span class="st">  </span><span class="kw">predict</span>(
  <span class="dt">object =</span> train_swag_svml, 
  <span class="dt">newdata =</span> x_test, 
  <span class="dt">type =</span> <span class="st">&quot;attribute&quot;</span>,
  <span class="dt">attribute =</span> 4L)


<span class="kw">sapply</span>(dim_pred,<span class="cf">function</span>(x) <span class="kw">head</span>(x))
<span class="co">#&gt; $predictions</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    1    1    1    1</span>
<span class="co">#&gt; [2,]    1    1    1    1</span>
<span class="co">#&gt; [3,]    1    1    1    1</span>
<span class="co">#&gt; [4,]    2    2    2    2</span>
<span class="co">#&gt; [5,]    1    1    1    1</span>
<span class="co">#&gt; [6,]    1    1    1    1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models</span>
<span class="co">#&gt; $models[[1]]</span>
<span class="co">#&gt; [1] 2 3 6 7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[2]]</span>
<span class="co">#&gt; [1] 2 3 5 6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[3]]</span>
<span class="co">#&gt; [1] 2 5 6 7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[4]]</span>
<span class="co">#&gt; [1] 3 5 6 7</span>

<span class="co"># predictions below a given CV error</span>

cv_pred &lt;-<span class="st">  </span><span class="kw">predict</span>(
  <span class="dt">object =</span> train_swag_svml, 
  <span class="dt">newdata =</span> x_test, 
  <span class="dt">type =</span> <span class="st">&quot;cv_performance&quot;</span>,
  <span class="dt">cv_performance =</span> <span class="fl">0.04</span>)

<span class="kw">sapply</span>(cv_pred,<span class="cf">function</span>(x) <span class="kw">head</span>(x))
<span class="co">#&gt; $predictions</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span>
<span class="co">#&gt; [1,]    1    1    1    1    1</span>
<span class="co">#&gt; [2,]    1    1    1    1    1</span>
<span class="co">#&gt; [3,]    1    1    1    1    1</span>
<span class="co">#&gt; [4,]    2    2    2    2    2</span>
<span class="co">#&gt; [5,]    1    1    1    1    1</span>
<span class="co">#&gt; [6,]    1    1    1    1    1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models</span>
<span class="co">#&gt; $models[[1]]</span>
<span class="co">#&gt; [1] 2 6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[2]]</span>
<span class="co">#&gt; [1] 2 6 7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[3]]</span>
<span class="co">#&gt; [1] 3 5 6</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[4]]</span>
<span class="co">#&gt; [1] 2 3 6 7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $models[[5]]</span>
<span class="co">#&gt; [1] 3 5 6 7</span></code></pre></div>
<p>Now we can for example evaluate the performance of the best learner selected by <strong>swag</strong> thanks to the <em>confusionMatrix()</em> function of <strong>caret</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># transform predictions into a data.frame of factors with levels of `y_test`</span>
best_learn &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">levels</span>(y_test)[best_pred<span class="op">$</span>predictions])
caret<span class="op">::</span><span class="kw">confusionMatrix</span>(best_learn,y_test)
<span class="co">#&gt; Confusion Matrix and Statistics</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;            Reference</span>
<span class="co">#&gt; Prediction  benign malignant</span>
<span class="co">#&gt;   benign        87         4</span>
<span class="co">#&gt;   malignant      3        42</span>
<span class="co">#&gt;                                           </span>
<span class="co">#&gt;                Accuracy : 0.9485          </span>
<span class="co">#&gt;                  95% CI : (0.8968, 0.9791)</span>
<span class="co">#&gt;     No Information Rate : 0.6618          </span>
<span class="co">#&gt;     P-Value [Acc &gt; NIR] : 6.123e-16       </span>
<span class="co">#&gt;                                           </span>
<span class="co">#&gt;                   Kappa : 0.8844          </span>
<span class="co">#&gt;                                           </span>
<span class="co">#&gt;  Mcnemar's Test P-Value : 1               </span>
<span class="co">#&gt;                                           </span>
<span class="co">#&gt;             Sensitivity : 0.9667          </span>
<span class="co">#&gt;             Specificity : 0.9130          </span>
<span class="co">#&gt;          Pos Pred Value : 0.9560          </span>
<span class="co">#&gt;          Neg Pred Value : 0.9333          </span>
<span class="co">#&gt;              Prevalence : 0.6618          </span>
<span class="co">#&gt;          Detection Rate : 0.6397          </span>
<span class="co">#&gt;    Detection Prevalence : 0.6691          </span>
<span class="co">#&gt;       Balanced Accuracy : 0.9399          </span>
<span class="co">#&gt;                                           </span>
<span class="co">#&gt;        'Positive' Class : benign          </span>
<span class="co">#&gt; </span></code></pre></div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
