points(df$LDH[index], rep(.06,length(index)), col = "blue", pch = 15, cex = 2)
index = which(df$`Sex M` == 0 & y_train == 1)
points(df$LDH[index], rep(.99,length(index)), col = "red", pch = 16, cex = 2)
index = which(df$`Sex M` == 1 & y_train == 1)
points(df$LDH[index], rep(.95,length(index)), col = "blue", pch = 16, cex = 2)
y_train = as.numeric(df$RÃ©a)
fit = glm(y_train~df$`Sex M`+df$LDH, family = binomial())
summary(fit)
pred = predict(fit, type="response")
plot(predict(fit, type="response"),pch = y_train)
range(df$LDH)
grid1 = 100:1700
tamer = (grid1*fit$coefficients[3] + fit$coefficients[1])
logit = 1/(1+exp(-tamer))
plot(grid1,logit, type = "l", col = "red")
tamer2 = grid1*fit$coefficients[3] + fit$coefficients[1] + fit$coefficients[2]
logit2 = 1/(1+exp(-tamer2))
lines(grid1,logit2, col = "blue")
index = which(df$`Sex M` == 0 & y_train == 0)
points(df$LDH[index], rep(0.02,length(index)), col = "red", pch = 15, cex = 2)
index = which(df$`Sex M` == 1 & y_train == 0)
points(df$LDH[index], rep(.06,length(index)), col = "blue", pch = 15, cex = 2)
index = which(df$`Sex M` == 0 & y_train == 1)
points(df$LDH[index], rep(.99,length(index)), col = "red", pch = 16, cex = 2)
index = which(df$`Sex M` == 1 & y_train == 1)
points(df$LDH[index], rep(.95,length(index)), col = "blue", pch = 16, cex = 2)
fit = glm(y_train~df$`Sex M`+df$LDH, family = binomial())
summary(fit)
pred = predict(fit, type="response")
plot(predict(fit, type="response"),pch = y_train)
test_logistis = seer(y, X, learner = "logistic")
# test code
require(caret)
require(doParallel)
y = round(runif(30))
X = matrix(rnorm(30*50),nrow = 30, ncol = 50)
test_logistis = seer(y, X, learner = "logistic")
train
learner = "glm"
family = "binomial"
metric = "Accuracy"
tuneGrid = NULL
preprocess = NULL
tuneLength = NULL
y = round(runif(30))
X = matrix(rnorm(30*50),nrow = 30, ncol = 50)
#10 fold CV repeated 10 times as
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
source('~/GitHub/SEER-R-Package/seer/R/seer.R', echo=TRUE)
train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
x <- as.matrix(X[,i])
i = 1
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
# is y a factor
if(!is.factor(y)){
y <- as.factor(y)
}
learn <- train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learner = "glm"
family = "binomial"
metric = NULL
tuneGrid = NULL
preprocess = NULL
tuneLength = NULL
learn <- train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
tunegrid
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
tunegrid
tunegrid = NULL
learn = train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn
1 - max(learn$results$Accuracy)
family = binomial()
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learner
family
y_train
y
family = "binomial"
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
df
str(df)
y <- as.factor(y)
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
metric = "Accuracy"
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn
1 - max(learn$results$Accuracy)
learner = "rf"
family = NULL
metric = "Accuracy"
tunegrid = expand.grid(.mtry=mtry)
family = NULL
preprocess = NULL
tuneLength = NULL
x <- as.matrix(X[,i])
mtry <- sqrt(ncol(x))
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
x <- as.matrix(X[,i])
mtry <- 1
df <- data.frame(y,x)
df
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
if(learn == "rf"){
mtry <- 1
tunegrid = expand.grid(.mtry=mtry)
}
expand.grid(.mtry=mtry)
if(learner == "rf"){
mtry <- 1
tunegrid = expand.grid(.mtry=mtry)
}
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
1 - max(learn$results$Accuracy)
learn
family = NULL
metric = "Accuracy"
tunegrid = NULL
preprocess = c("center", "scale")
tuneLength = 10
learner = "svmLinear"
if(learner == "rf"){
mtry <- 1
tunegrid = expand.grid(.mtry=mtry)
}
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
learn
1 - max(learn$results$Accuracy)
devtools::document()
p = 30
r= .1
m = 40000
(1+sqrt(1+8*m/r))/(2*p)
# test code
require(caret)
require(doParallel)
y = round(runif(30))
X = matrix(rnorm(30*50),nrow = 30, ncol = 50)
test_logistis = seer(y, X, learner = "logistic")
library(seer)
# test code
require(caret)
require(doParallel)
y = round(runif(30))
X = matrix(rnorm(30*50),nrow = 30, ncol = 50)
test_logistis = seer(y, X, learner = "logistic")
devtools::document()
library(seer)
devtools::document()
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
library(seer)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
metera_in_logistic = seer(y_train, X_train,learner = "logistic",dmax = 7,m = 40000)
source('~/.active-rstudio-document', echo=TRUE)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 7,m = 40000)
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 2,m = 40000)
y = y_train
X = x_train
learner = "logistic"
dmax = 2
m = 40000
if(is.null(learner)){
stop("No learning method specified. Please specify a `learner`")
}else{
if(learner == "logistic"){
learner = "glm"
family = "binomial"
metric = "Accuracy"
tunegrid = NULL
preprocess = NULL
tuneLength = NULL
}else if(learner == "rf"){
family = NULL
metric = "Accuracy"
family = NULL
preprocess = NULL
tuneLength = NULL
}else{
family = NULL
metric = "Accuracy"
tunegrid = NULL
preprocess = c("center", "scale")
tuneLength = 10
}
}
if(is.null(y)){
stop("Please provide a response vector `y`")
}
if(is.null(X)){
stop("Please provide an attributes matrix `X`")
}else{
if(!is.matrix(X)){
stop("X must be a matrix")
}
}
## object dimension
# Number of attributes
p <- ncol(X)
# Number of observations
n <- length(y)
# is y a factor
if(!is.factor(y)){
y <- as.factor(y)
}
# Define parallelisation parameter
if(isTRUE(parallel_comput)){
if(is.null(nc)){
nc = detectCores()
}else{
nc = nc
}
cl <- makePSOCKcluster(nc)
registerDoParallel(cl)
}
parallel_comput = T
# Define parallelisation parameter
if(isTRUE(parallel_comput)){
if(is.null(nc)){
nc = detectCores()
}else{
nc = nc
}
cl <- makePSOCKcluster(nc)
registerDoParallel(cl)
}
nc = NULL
# Define parallelisation parameter
if(isTRUE(parallel_comput)){
if(is.null(nc)){
nc = detectCores()
}else{
nc = nc
}
cl <- makePSOCKcluster(nc)
registerDoParallel(cl)
}
# Maximum number of attributes per learner
if(is.null(m)){
m = 40000
}
# Maximum number of attributes per learner
if(is.null(dmax)){
event = as.numeric(as.character(y))
dmax <- ceiling(min(sum(event),n-sum(event))/p)
if(dmax == 1){
dmax = 3
}
}
dmax
## Seed
set.seed(seed)
graine <- sample.int(1e6,dmax)
## Object storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
seed = 666
## Seed
set.seed(seed)
graine <- sample.int(1e6,dmax)
## Object storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
VarMat <- vector("list",dmax)
cv_errors <- times <- vector("numeric",p)
#10 fold CV repeated 10 times as
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
cv_errors <- times <- vector("numeric",p)
#10 fold CV repeated 10 times as
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
for(i in seq_len(p)){
if(learner == "rf"){
mtry <- 1
tunegrid = expand.grid(.mtry=mtry)
}
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
cv_errors[i] = 1 - max(learn$results$Accuracy)
}
cv_errors
CVs[[1]] <- cv_errors
VarMat[[1]] <- seq_along(cv_errors)
cv_errors <- cv_errors[!is.na(cv_errors)]
cv_errors
q0_test =  which(cv_errors <= quantile(cv_errors,0.01))
q0_test
q0_test =  which(cv_errors <= quantile(cv_errors,0.01))
if(length(q0_test) < 30){
q0 = 0.05
}else{
q0 = 0.01
}
IDs[[1]] <- which(cv_errors <= quantile(cv_errors,q0))
id_screening <- IDs[[1]]
id_screening
for(d in 2:dmax){
# cv0 <- cv1
idRow <- IDs[[d-1]]
if(d==2){
idVar <- VarMat[[d-1]][idRow]
nrv <- length(idVar)
}else{
idVar <- VarMat[[d-1]][idRow,]
nrv <- nrow(idVar)
}
# build all possible
A <- matrix(nr=nrv*length(id_screening),nc=d)
A[,1:(d-1)] <- kronecker(cbind(rep(1,length(id_screening))),idVar)
A[,d] <- rep(id_screening,each=nrv)
B <- unique(t(apply(A,1,sort)))
id_ndup <- which(apply(B,1,anyDuplicated) == 0)
var_mat <- B[id_ndup,]
rm(list=c("A","B"))
if(nrow(var_mat)>m){
set.seed(graine[d]+1)
VarMat[[d]] <- var_mat[sample.int(nrow(var_mat),m),]
}else{
VarMat[[d]] <- var_mat
}
var_mat <- VarMat[[d]]
cv_errors <- rep(NA,nrow(var_mat))
if(learner == "rf"){
mtry <- d
tunegrid = expand.grid(.mtry=mtry)
}
for(i in seq_len(nrow(var_mat))){
rc <- var_mat[i,]
seed <- graine[d] + i
x <- as.matrix(X[,rc])
mtry <- sqrt(ncol(x))
breast_1 = data.frame(y,x)
learn = train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
cv_errors[i] = 1 - max(learn$results$Accuracy)
}
attr(cv_errors,"rng") <- NULL
CVs[[d]] <- cv_errors
cv1 <- quantile(cv_errors,probs=q0,na.rm=T)
IDs[[d]] <- which(cv_errors<=cv1)
}
IDs
CVs
stopCluster(cl)
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
mod_size_min_med
#quantile of the 1% most predictive models
treshold_seer_set = quantile(CVs[[mod_size_min_med]],seq(0, 1, 0.01))[2]
treshold_seer_set
# Vector of models dimension
dim_model = 1:dmax
dim_model
# Find the index of model selected
index_model_select = list()
for(d in seq_len(dmax)){
if(sum(CVs[[dim_model[d]]] <=treshold_seer_set) == 0){
index_model_select[[d]] = "empty"
}else{
index_model_select[[d]] = which(CVs[[dim_model[d]]] <=treshold_seer_set)
}
}
index_model_select
# vector od model dimension selected
model_dim_selected = which(index_model_select != "empty")
model_dim_selected
## create output for SEER subset
## index of variable selected and respective cv error
seer_model = list()
seer_cv_error = list()
for(d in seq_along(model_dim_selected)){
index_mod = model_dim_selected[[d]]
# model seer set
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[index_mod]],]
# cv error
seer_cv_error[[d]] <- CVs[[index_mod]][index_model_select[[index_mod]]]
}
seq_along(model_dim_selected)
VarMat[[index_mod]]
index_model_select
VarMat[[index_mod]][index_model_select[[d]][[index_mod]],]
VarMat[[index_mod]]
model_dim_selected[[d]]
VarMat[[index_mod]][index_model_select[[d]][[index_mod]]]
for(d in seq_along(model_dim_selected)){
index_mod = model_dim_selected[[d]]
if(d == 1){
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[d]][[index_mod]]]
}else{
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[d]][[index_mod]],]
}
seer_cv_error[[d]] <- CVs[[index_mod]][index_model_select[[d]][[index_mod]]]
}
seer_cv_error
seer_model
#
table_variable = table(unlist(seer_model))
variable_index = as.numeric(names(table_variable))
names(table_variable) = colnames(X[,variable_index])
table_variable
variable_index
table_variable
names(table_variable) = colnames(X[,variable_index])
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 4,m = 40000)
library(seer)
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 3,m = 40000)
metera_in_logistic$pred_cv
metera_in_logistic$table_variable
metera_in_logistic$model_selected
CVs = metera_in_logistic$pred_cv
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
dmax = 3
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
mod_size_min_med
#quantile of the 1% most predictive models
treshold_seer_set = quantile(CVs[[mod_size_min_med]],seq(0, 1, 0.01))[2]
treshold_seer_set
CVs
m_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[4])
l_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[1])
u_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[6])
require(plotrix)
plotCI(1:dmax, m_vector, ui=u_vector, li=l_vector, scol = "grey", col="red"
, pch = 16, main = "Set of Highly Predictive Models - III Cartesian Quadrant",ylab = "Range CV Error",xlab = "Model Size")
mod_size_min = which.min(unlist(lapply(CVs[1:dmax], min)))
abline(v = mod_size_min, col="blue",lwd=2)
ab = quantile(CVs[[mod_size_min]],seq(0, 1, 0.01))[2]
abline(h = ab, col="blue",lwd=2)
library(seer)
library(seer)
