nc = detectCores()
}else{
nc = nc
}
cl <- makePSOCKcluster(nc)
registerDoParallel(cl)
}
nc = NULL
# Define parallelisation parameter
if(isTRUE(parallel_comput)){
if(is.null(nc)){
nc = detectCores()
}else{
nc = nc
}
cl <- makePSOCKcluster(nc)
registerDoParallel(cl)
}
# Maximum number of attributes per learner
if(is.null(m)){
m = 40000
}
# Maximum number of attributes per learner
if(is.null(dmax)){
event = as.numeric(as.character(y))
dmax <- ceiling(min(sum(event),n-sum(event))/p)
if(dmax == 1){
dmax = 3
}
}
dmax
## Seed
set.seed(seed)
graine <- sample.int(1e6,dmax)
## Object storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
seed = 666
## Seed
set.seed(seed)
graine <- sample.int(1e6,dmax)
## Object storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
VarMat <- vector("list",dmax)
cv_errors <- times <- vector("numeric",p)
#10 fold CV repeated 10 times as
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
cv_errors <- times <- vector("numeric",p)
#10 fold CV repeated 10 times as
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
for(i in seq_len(p)){
if(learner == "rf"){
mtry <- 1
tunegrid = expand.grid(.mtry=mtry)
}
x <- as.matrix(X[,i])
df <- data.frame(y,x)
learn <- train(y ~., data = df, method = learner,metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
cv_errors[i] = 1 - max(learn$results$Accuracy)
}
cv_errors
CVs[[1]] <- cv_errors
VarMat[[1]] <- seq_along(cv_errors)
cv_errors <- cv_errors[!is.na(cv_errors)]
cv_errors
q0_test =  which(cv_errors <= quantile(cv_errors,0.01))
q0_test
q0_test =  which(cv_errors <= quantile(cv_errors,0.01))
if(length(q0_test) < 30){
q0 = 0.05
}else{
q0 = 0.01
}
IDs[[1]] <- which(cv_errors <= quantile(cv_errors,q0))
id_screening <- IDs[[1]]
id_screening
for(d in 2:dmax){
# cv0 <- cv1
idRow <- IDs[[d-1]]
if(d==2){
idVar <- VarMat[[d-1]][idRow]
nrv <- length(idVar)
}else{
idVar <- VarMat[[d-1]][idRow,]
nrv <- nrow(idVar)
}
# build all possible
A <- matrix(nr=nrv*length(id_screening),nc=d)
A[,1:(d-1)] <- kronecker(cbind(rep(1,length(id_screening))),idVar)
A[,d] <- rep(id_screening,each=nrv)
B <- unique(t(apply(A,1,sort)))
id_ndup <- which(apply(B,1,anyDuplicated) == 0)
var_mat <- B[id_ndup,]
rm(list=c("A","B"))
if(nrow(var_mat)>m){
set.seed(graine[d]+1)
VarMat[[d]] <- var_mat[sample.int(nrow(var_mat),m),]
}else{
VarMat[[d]] <- var_mat
}
var_mat <- VarMat[[d]]
cv_errors <- rep(NA,nrow(var_mat))
if(learner == "rf"){
mtry <- d
tunegrid = expand.grid(.mtry=mtry)
}
for(i in seq_len(nrow(var_mat))){
rc <- var_mat[i,]
seed <- graine[d] + i
x <- as.matrix(X[,rc])
mtry <- sqrt(ncol(x))
breast_1 = data.frame(y,x)
learn = train(y ~., data = df, method = learner, metric = metric, family = family,
trControl=trctrl, preProcess = preprocess, tuneLength = tuneLength,
tuneGrid=tunegrid)
cv_errors[i] = 1 - max(learn$results$Accuracy)
}
attr(cv_errors,"rng") <- NULL
CVs[[d]] <- cv_errors
cv1 <- quantile(cv_errors,probs=q0,na.rm=T)
IDs[[d]] <- which(cv_errors<=cv1)
}
IDs
CVs
stopCluster(cl)
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
mod_size_min_med
#quantile of the 1% most predictive models
treshold_seer_set = quantile(CVs[[mod_size_min_med]],seq(0, 1, 0.01))[2]
treshold_seer_set
# Vector of models dimension
dim_model = 1:dmax
dim_model
# Find the index of model selected
index_model_select = list()
for(d in seq_len(dmax)){
if(sum(CVs[[dim_model[d]]] <=treshold_seer_set) == 0){
index_model_select[[d]] = "empty"
}else{
index_model_select[[d]] = which(CVs[[dim_model[d]]] <=treshold_seer_set)
}
}
index_model_select
# vector od model dimension selected
model_dim_selected = which(index_model_select != "empty")
model_dim_selected
## create output for SEER subset
## index of variable selected and respective cv error
seer_model = list()
seer_cv_error = list()
for(d in seq_along(model_dim_selected)){
index_mod = model_dim_selected[[d]]
# model seer set
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[index_mod]],]
# cv error
seer_cv_error[[d]] <- CVs[[index_mod]][index_model_select[[index_mod]]]
}
seq_along(model_dim_selected)
VarMat[[index_mod]]
index_model_select
VarMat[[index_mod]][index_model_select[[d]][[index_mod]],]
VarMat[[index_mod]]
model_dim_selected[[d]]
VarMat[[index_mod]][index_model_select[[d]][[index_mod]]]
for(d in seq_along(model_dim_selected)){
index_mod = model_dim_selected[[d]]
if(d == 1){
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[d]][[index_mod]]]
}else{
seer_model[[d]] <- VarMat[[index_mod]][index_model_select[[d]][[index_mod]],]
}
seer_cv_error[[d]] <- CVs[[index_mod]][index_model_select[[d]][[index_mod]]]
}
seer_cv_error
seer_model
#
table_variable = table(unlist(seer_model))
variable_index = as.numeric(names(table_variable))
names(table_variable) = colnames(X[,variable_index])
table_variable
variable_index
table_variable
names(table_variable) = colnames(X[,variable_index])
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 4,m = 40000)
library(seer)
metera_in_logistic = seer(y_train, x_train,learner = "logistic",dmax = 3,m = 40000)
metera_in_logistic$pred_cv
metera_in_logistic$table_variable
metera_in_logistic$model_selected
CVs = metera_in_logistic$pred_cv
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
dmax = 3
## Define the seer sets of models
# Dimension which minimize the median cv error at each dimesion
mod_size_min_med = which.min(unlist(lapply(CVs[1:dmax], median)))
mod_size_min_med
#quantile of the 1% most predictive models
treshold_seer_set = quantile(CVs[[mod_size_min_med]],seq(0, 1, 0.01))[2]
treshold_seer_set
CVs
m_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[4])
l_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[1])
u_vector <- sapply(CVs[c(1:dmax)], function(x) summary(x)[6])
require(plotrix)
plotCI(1:dmax, m_vector, ui=u_vector, li=l_vector, scol = "grey", col="red"
, pch = 16, main = "Set of Highly Predictive Models - III Cartesian Quadrant",ylab = "Range CV Error",xlab = "Model Size")
mod_size_min = which.min(unlist(lapply(CVs[1:dmax], min)))
abline(v = mod_size_min, col="blue",lwd=2)
ab = quantile(CVs[[mod_size_min]],seq(0, 1, 0.01))[2]
abline(h = ab, col="blue",lwd=2)
library(seer)
library(seer)
library(seer)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
require(doParallel)
require(caret)
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
library(seer)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
require(doParallel)
require(caret)
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
y = y_train
X = x_train
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
library(seer)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
require(doParallel)
require(caret)
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
y = y_train
X = x_train
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
library(seer)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
require(doParallel)
require(caret)
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
y = y_train
X = x_train
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
load("/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
require(doParallel)
require(caret)
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
y = y_train
X = x_train
metera_in_logistic = seer(y = y_train, X = x_train,learner = "svmLinear",dmax = 6,m = 40000,q0=0.05)
metera_in_logistic
metera_in_logistic$pred_cv
dmax = 6
m_vector <- sapply(metera_in_logistic$pred_cv[c(1:dmax)], function(x) summary(x)[4])
l_vector <- sapply(metera_in_logistic$pred_cv[c(1:dmax)], function(x) summary(x)[1])
u_vector <- sapply(metera_in_logistic$pred_cv[c(1:dmax)], function(x) summary(x)[6])
require(plotrix)
plotCI(1:dmax, m_vector, ui=u_vector, li=l_vector, scol = "grey", col="red"
, pch = 16, main = "Set of Highly Predictive Models - III Cartesian Quadrant",ylab = "Range CV Error",xlab = "Model Size")
save(metera_in_logistic , file = "metera_in_logistic.rda")
require(caret)
require(doParallel)
## Load Data
setwd("~/Github/meta-learning/Meter A")
load("data_split.rda")
## Creat data meter a
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
## Meta parameters
q0 <- .05 # quantile for screening
dmax <- 6 # max model size (Sam stops at models with 10 predictors)
mod_max <- 4e4 # model explored at each step
# data storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
VarMat <- vector("list",dmax)
set.seed(163L)
graine <- sample.int(1e6,dmax)
##
# Initial step: (dimension 1)
# EXAMPLE FOR D=1
cv_errors <- vector("numeric",ncol(x_train))
nc = detectCores()
cl <- makeCluster(cl)
registerDoParallel(cl)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 1) #10 fold CV repeated 10 times as PANNING
cv_errors <- foreach(i = seq_along(cv_errors), .combine = c, .packages=c("caret")) %dopar% {
seed <- graine[1] + i
X <- as.matrix(x_train[,i])
y <- as.factor(y_train)
breast_1 = data.frame(y,X)
obj = train(y ~., data = breast_1, method = "svmLinear", trControl=trctrl, preProcess = c("center", "scale"),tuneLength = 10)
cv_errors = 1 - max(obj$results$Accuracy)
}
stopCluster(cl)
CVs[[1]] <- cv_errors
VarMat[[1]] <- seq_along(cv_errors)
cv_errors <- cv_errors[!is.na(cv_errors)]
IDs[[1]] <- which(cv_errors <= quantile(cv_errors,q0))
save(CVs,file="CVs_svml_metera.rda")
save(IDs,file="IDs_svml_metera.rda")
save(VarMat,file="VarMat_svml_metera.rda") #each row of VarMat gives me the tested regressors (at most 40000)
id_screening <- IDs[[1]]
# Dimension d >= 2
for(d in 2:dmax){
# cv0 <- cv1
idRow <- IDs[[d-1]]
if(d==2){
idVar <- VarMat[[d-1]][idRow]
nrv <- length(idVar)
}else{
idVar <- VarMat[[d-1]][idRow,]
nrv <- nrow(idVar)
}
# build all possible
A <- matrix(nr=nrv*length(id_screening),nc=d)
A[,1:(d-1)] <- kronecker(cbind(rep(1,length(id_screening))),idVar)
A[,d] <- rep(id_screening,each=nrv)
B <- unique(t(apply(A,1,sort)))
id_ndup <- which(apply(B,1,anyDuplicated) == 0)
var_mat <- B[id_ndup,]
rm(list=c("A","B"))
if(nrow(var_mat)>mod_max){
set.seed(graine[d]+1)
VarMat[[d]] <- var_mat[sample.int(nrow(var_mat),mod_max),]
}else{
VarMat[[d]] <- var_mat
}
var_mat <- VarMat[[d]]
cv_errors <- rep(NA,nrow(var_mat))
nc = detectCores()
cl <- makeCluster(nc)
registerDoParallel(cl)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 1) #10 fold CV repeated 10 times as PANNING
cv_errors <- foreach(i = seq_along(cv_errors), .combine = c, .packages=c("caret")) %dopar% {
rc <- var_mat[i,]
seed <- graine[d] + i
X <- as.matrix(x_train[,rc])
y <- as.factor(y_train)
breast_1 = data.frame(y,X)
obj = train(y ~., data = breast_1, method = "svmLinear", trControl=trctrl, preProcess = c("center", "scale"),tuneLength = 10)
cv_errors = 1 - max(obj$results$Accuracy)
}
stopCluster(cl)
attr(cv_errors,"rng") <- NULL
CVs[[d]] <- cv_errors
cv1 <- quantile(cv_errors,probs=q0,na.rm=T)
IDs[[d]] <- which(cv_errors<=cv1)
save(CVs,file="CVs_svml_metera.rda")
save(IDs,file="IDs_svml_metera.rda")
save(VarMat,file="VarMat_svml_metera.rda")
print(d)
}
## Load Data
setwd("~/Github/meta-learning/Meter A")
load("~/Github/meta-learning/Meter A/data_split.rda")
load("~/Users/gaetan/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
## Load Data
load("~/GitHub/meta-learning/Datasets/Meter A/data_split.rda")
y_train = as.factor(df_metera$y_train)
y_test = as.factor(df_metera$y_test)
x_train = df_metera$x_train
x_test = df_metera$x_test
x_train <- as.matrix(model.matrix( ~.^2, data=df_metera$x_train))
x_train <- x_train[,-1]
x_test = as.matrix(model.matrix( ~.^2, data=df_metera$x_test))
x_test <- x_test[,-1]
## Meta parameters
q0 <- .05 # quantile for screening
dmax <- 6 # max model size (Sam stops at models with 10 predictors)
mod_max <- 4e4 # model explored at each step
# data storage
CVs <- vector("list",dmax)
IDs <- vector("list",dmax)
VarMat <- vector("list",dmax)
set.seed(163L)
graine <- sample.int(1e6,dmax)
# Initial step: (dimension 1)
# EXAMPLE FOR D=1
cv_errors <- vector("numeric",ncol(x_train))
nc = detectCores()
cl <- makeCluster(cl)
registerDoParallel(cl)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 1) #10 fold CV repeated 10 times as PANNING
cv_errors <- foreach(i = seq_along(cv_errors), .combine = c, .packages=c("caret")) %dopar% {
seed <- graine[1] + i
X <- as.matrix(x_train[,i])
y <- as.factor(y_train)
breast_1 = data.frame(y,X)
obj = train(y ~., data = breast_1, method = "svmLinear", trControl=trctrl, preProcess = c("center", "scale"),tuneLength = 10)
cv_errors = 1 - max(obj$results$Accuracy)
}
stopCluster(cl)
CVs[[1]] <- cv_errors
VarMat[[1]] <- seq_along(cv_errors)
require(doParallel)
cv_errors
IDs[[1]] <- which(cv_errors <= quantile(cv_errors,q0))
CVs[[1]] <- cv_errors
VarMat[[1]] <- seq_along(cv_errors)
cv_errors <- cv_errors[!is.na(cv_errors)]
IDs[[1]] <- which(cv_errors <= quantile(cv_errors,q0))
save(CVs,file="CVs_svml_metera2.rda")
save(IDs,file="IDs_svml_metera2.rda")
save(VarMat,file="VarMat_svml_metera2.rda") #each row of VarMat gives me the tested regressors (at most 40000)
id_screening <- IDs[[1]]
for(d in 2:dmax){
# cv0 <- cv1
idRow <- IDs[[d-1]]
if(d==2){
idVar <- VarMat[[d-1]][idRow]
nrv <- length(idVar)
}else{
idVar <- VarMat[[d-1]][idRow,]
nrv <- nrow(idVar)
}
# build all possible
A <- matrix(nr=nrv*length(id_screening),nc=d)
A[,1:(d-1)] <- kronecker(cbind(rep(1,length(id_screening))),idVar)
A[,d] <- rep(id_screening,each=nrv)
B <- unique(t(apply(A,1,sort)))
id_ndup <- which(apply(B,1,anyDuplicated) == 0)
var_mat <- B[id_ndup,]
rm(list=c("A","B"))
if(nrow(var_mat)>mod_max){
set.seed(graine[d]+1)
VarMat[[d]] <- var_mat[sample.int(nrow(var_mat),mod_max),]
}else{
VarMat[[d]] <- var_mat
}
var_mat <- VarMat[[d]]
cv_errors <- rep(NA,nrow(var_mat))
nc = detectCores()
cl <- makeCluster(nc)
registerDoParallel(cl)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 1) #10 fold CV repeated 10 times as PANNING
cv_errors <- foreach(i = seq_along(cv_errors), .combine = c, .packages=c("caret")) %dopar% {
rc <- var_mat[i,]
seed <- graine[d] + i
X <- as.matrix(x_train[,rc])
y <- as.factor(y_train)
breast_1 = data.frame(y,X)
obj = train(y ~., data = breast_1, method = "svmLinear", trControl=trctrl, preProcess = c("center", "scale"),tuneLength = 10)
cv_errors = 1 - max(obj$results$Accuracy)
}
stopCluster(cl)
attr(cv_errors,"rng") <- NULL
CVs[[d]] <- cv_errors
cv1 <- quantile(cv_errors,probs=q0,na.rm=T)
IDs[[d]] <- which(cv_errors<=cv1)
save(CVs,file="CVs_svml_metera2.rda")
save(IDs,file="IDs_svml_metera2.rda")
save(VarMat,file="VarMat_svml_metera2.rda")
print(d)
}
